---
output:
  html_document:
    css: css/styles.css
---

```{r setup, echo=FALSE, warning=FALSE, purl=FALSE, message=FALSE}
options(repos = "http://cran.rstudio.com/")
pkgs <- c("dplyr", "knitr")
x<-lapply(pkgs, library, character.only = TRUE)
opts_chunk$set(tidy = FALSE, message = F, warning = F)
```

<script src="js/hideoutput.js"></script>

# Spatial data analysis and mapping

Get the lesson R script: [mapping.R](mapping.R)

Get the lesson data: [download zip](data/data.zip)

## Lesson Outline

* [Vector data]
* [Simple features]
* [Creating spatial data with simple features]
* [Basic geospatial analysis]
* [Quick mapping]

## Lesson Exercises

* [Exercise 12]
* [Exercise 13]
* [Exercise 14]

R has been around for over twenty years and most of its use has focused on statistical analysis.  Tools for spatial analysis have been developed in recent years that allow the use of R as a full-blown GIS with capabilities similar or even superior to commerical software.  This lesson will focus on geospatial analysis using the [simple features](https://r-spatial.github.io/sf/){target="_blank"} package.  We will focus entirely on working with vector data in this lesson, but checkout the [raster](https://cran.r-project.org/web/packages/raster/){target="_blank"} and [rgdal](https://cran.r-project.org/web/packages/rgdal/index.html){target="_blank"} packages if you want to work with raster data in R.  There are several useful vignettes in the raster link.

The goals for today are:

1) Understand the vector data structure

1) Understand how to import and structure vector data in R

1) Understand how R stores spatial data using the simple features package

1) Execute basic geospatial functions in R

## Vector data

Most of us are probably familiar with the basic types of spatial data and their components.  We're going to focus entirely on vector data for this lesson because these data are easily conceptualized as __features__ or discrete objects with spatial information.  We'll discuss some of the details about this later.  Raster data by contrast are stored in a regular grid where the cells of the grid are associated with values.  Raster data are more common for data with continuous coverage, such as climate or weather layers.  

Vector data come in three flavors.  The simplest is a __point__, which is a 0-dimensional feature that can be used to represent a specific location on the earth, such as a single tree or an entire city. Linear, 1-dimensional features can be represented with points (or vertices) that are connected by a path to form a __line__ and when many points are connected these form a __polyline__. Finally, when a polyline's path returns to its origin to represent an enclosed 2-dimensional space, such as a forest, watershed boundary, or lake, this forms a __polygon__.

![https://earthdatascience.org/courses/earth-analytics/spatial-data-r/intro-vector-data-r/](figure/pts-lines-polys.png)

*Image [source](https://earthdatascience.org/courses/earth-analytics/spatial-data-r/intro-vector-data-r/){target="_blank"}*

All vector data are represented similarly, whether they're points, lines or polygons.  Points are defined by a single coordinate location, whereas a line or polygon is several points with a grouping variable that distinguishes one object from another. In all cases, the aggregate dataset is composed of one or more features of the same type (points, lines, or polygons).

There are two other pieces of information that are included with vector data.  The __attributes__ that can be associated with each feature and the __coordinate reference system__ or __CRS__.  The attributes can be any supporting information about a feature, such as a text description or summary data about the features.  You can identify attributes as anything in a spatial dataset that is not explicitly used to define the location of the features.  

The CRS is used to establish a frame of reference for the locations in your spatial data.  The chosen CRS ensures that all features are correctly referenced relative to each other, especially between different datasets.  As a simple example, imagine comparing length measurements for two objects where one was measured in centimeters and another in inches.  If you didn't know the unit of measurement, you could not compare relative lengths.  The CRS is similar in that it establishes a common frame of reference, but for spatial data.  An added complication with spatial data is that location can be represented in both 2-dimensional or 3-dimensional space. This is beyond the scope of this lesson, but for any geospatial analysis you should be sure that:

1) the CRS is the same when comparing datasets, and 

1) the CRS is appropriate for the region you're looking at.    

![](figure/crs-comparisons.jpg) 

*Image [source](https://nceas.github.io/oss-lessons/spatial-data-gis-law/1-mon-spatial-data-intro.html){target="_blank"}*

To summarize, vector data include the following:

1) spatial data (e.g., latitude, longitude) as points, lines, or polygons

1) attributes

1) a coordinate reference system

These are all the pieces of information you need to recognize in your data when working with features in R.

## Simple features

R has a long history of packages for working with spatial data.  For many years, the [sp](https://cran.r-project.org/web/packages/sp/index.html){target="_blank"} package was the standard and most widely used toolset for working with spatial data in R. This package laid the foundation for creating spatial data classes and methods in R, but unfortunately it's development predated a lot of the newer tools that are built around the [tidyverse](https://www.tidyverse.org/){target="_blank"}.  This makes it incredibly difficult to incorporate `sp` data objects with these newer data analysis workflows.  

The [simple features](https://r-spatial.github.io/sf/){target="_blank"} or sf package was developed to streamline the use of spatial data in R and to align its functionality with those provided in the tidyverse.  The sf package is already beginning to replace sp as the fundamental spatial model in R for vector data.  A major advantage of sf, as you'll see, is its intuitive data structure that retains many familiar components of the `data.frame` (or more accurately, `tibble`).

Simple Features is a hierarchical data model that represents a wide range of geometry types - it includes all common vector geometry types (but does not include raster) and even allows geometry collections, which can have multiple geometry types in a single object. From the first sf package vignette we see:

![](figure/sf_objects.png)

You'll notice that these are the same features we described above, with the addition of "multi" features and geometry collections that include more than one type of feature.

## Exercise 12

Let's get setup for this lesson.  We'll make sure we have the necessary packages installed and loaded.  Then we'll import our datasets. 

1. Open a new script in your RStudio project or within RStudio cloud.  

1. At the top of the script, load the `tidyverse`, `sf`, and `mapview` libraries.  Don't forget you can use `install.packages(c('tidyverse', 'sf', 'mapview'))` if the packages aren't installed.

1. Load the `fishdat.csv`, `statloc.csv`, and `sgdat.shp` datasets from your data folder.  For the csv files, use `read_csv()` and for the shapefile use, use the `st_read()` function from the `sf` package.  The shapefile is seagrass polygon data for Old Tampa Bay in 2016. As before, assign each loaded dataset to an object in your workspace.  

<div class="fold s o">
```{r results = 'hide'}
# load libraries
library(tidyverse)
library(sf)
library(mapview)

# load the fish data
fishdat <- read_csv('data/fishdat.csv')

# load the station data
statloc <- read_csv('data/statloc.csv')

# load the sgdat shapefile
sgdat <- st_read('data/sgdat.shp')
```
</div>

## Creating spatial data with simple features

Now that we're setup, let's talk about how the `sf` package can be used.  After the package is loaded, you can check out all of the methods that are available for `sf` data objects.  Many of these names will look familiar if you've done geospatial analysis before.  We'll use some of these a little bit later.

```{r}
methods(class = 'sf')
```

All of the functions and methods in sf are prefixed with st_, which stands for ‘spatial and temporal’.  This is kind of confusing but this is in reference to standard methods avialable in [PostGIS](https://en.wikipedia.org/wiki/PostGIS){target="_blank"}, an open-source backend that is used by many geospatial platforms.  An advantage of this prefixing is all commands are easy to find with command-line completion in sf, in addition to having naming continuity with existing software.

There are two ways to create a spatial data object in R, i.e., an `sf` object, using the sf package.

1) Directly import a shapefile

1) Convert an existing R object with latitude/longitude data that represent point features

We've already imported a shapefile in the first exercise, so let's look at its structure to better understand the `sf` object. The `st_read()` function can be used for import.  Setting `quiet = T` will keep R from being chatty when it imports the data.

```{r}
sgdat <- st_read('data/sgdat.shp', quiet = T)
sgdat
```

What does this show us? Let's break it down.

![](figure/sf_struct.png)

* In green, metadata describing components of the `sf` object
* In yellow, a simple feature: a single record, or `data.frame` row, consisting of attributes and geometry
* In blue, a single simple feature geometry (an object of class `sfg`)
* In red, a simple feature list-column (an object of class `sfc`, which is a column in the data.frame)

We've just imported a polygon dataset with 575 features and 2 fields.  The dataset is projected using the WGS 84 geographic CRS (i.e., latitude/longitude).  You'll notice that the actual dataset looks very similar to a regular `data.frame`, with some interesting additions.  The header includes some metadata about the `sf` object and the `geometry` column includes the actual spatial information for each feature.  Conceptually, you can treat the `sf` object like you would a `data.frame`.   

Easy enough, but what if we have point data that's not a shapefile?  You can create an `sf` object from any existing `data.frame` so long as the data include coordinate information (e.g., columns for longitude and latitude) and you know the CRS (or can make an educated guess).  We can do this with our `fishdat` and `statloc` csv files that we imported. 

```{r}
str(fishdat)
str(statloc)
```

The `st_as_sf()` function can be used to make this `data.frame` into a `sf` object, but we have first have to join the two datasets and tell R which column is the x-coordinates and which is the y-coordinates.  We also have to specify a CRS - this is just a text string or number (i.e, EPSG)  in a standard format for geospatial data. A big part of working with spatial data is keeping track of reference systems between different datasets.  Remember that meaningful comparisons between datasets are only possible if the CRS is shared.  

There are many, many types of reference systems and plenty of resources online that provide detailed explanations of the what and why behind the CRS (see [spatialreference.org](http://www.spatialreference.org/){target="_blank"} or [this guide](https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/OverviewCoordinateReferenceSystems.pdf){target="_blank"} from NCEAS).  For now, just realize that we can use a simple text string in R to indicate which CRS we want.  Although this may not always be true, we can make an educated guess that the standard geographic projection with the WGS84 datum applies to our dataset.

## Exercise 13

Let's join the `fishdat` and `statloc` datasets and create an `sf` object using `st_as_sf()` function.  

1. Join `fishdat` to `statloc` using the `left_join()` function with `by = "Reference"` as the key. Verify the joined dataset has the same number of observations as the original (hint: use `nrow` to check the number of rows before/after joining). 

1. Use `st_as_sf()` to make the joined dataset an `sf` object.  There are two arguments you need to specifiy with `st_as_sf()`: `coords = c('Longitude', 'Latitude')` so R knows which columns are the x/y coordinates and `crs = 4326` to specifiy the CRS as WGS 84.  

1. When you're done, inspect the dataset.  How many features are there?  What type of spatial object is this? 

<div class="fold s o">
```{r results = 'hide'}
# Join the data
alldat <- left_join(fishdat, statloc, by = 'Reference')

# check row counts
nrow(fishdat)
nrow(statloc)
nrow(alldat)

# create spatial data object
alldat <- st_as_sf(alldat, coords = c('Longitude', 'Latitude'), crs = 4326)

# examine the sf objec
alldat
str(alldat)
```
</div>

There's a shortcut to specifying the CRS if you don't know which one to use.  Remember, for spatial anlaysis make sure to only work with datasets that have the same projections and coordinate systems.  The `st_crs()` function tells us the CRS for a dataset. 

```{r}
# check crs
st_crs(alldat)
```

When we created the `alldat` dataset, we could have used the CRS from the `sgdat` object that we created in the first exercise by using `crs = st_crs(sgdat)` for the `crs` argument.  This is often a quick shortcut for creating an `sf` object without having to look up the CRS number.  

We can verify that both are now have the same CRS as WGS 84.

```{r}
# verify the polygon and point data have the same crs
st_crs(sgdat)
st_crs(alldat)
```

Finally, you may often want to use another coordinate system, such as a projection that is regionally-specific.  You can use the `st_transform()` function to quickly change and/or reproject an `sf` object.  For example, if we want to convert a geographic to UTM projection: 

```{r}
alldatutm <- alldat %>% 
  st_transform(crs = '+proj=utm +zone=17 +datum=NAD83 +units=m +no_defs')
st_crs(alldatutm)
```

## Basic geospatial analysis

As with any analysis, let's take a look at the data to see what we're dealing with before we start comparing the two.  

```{r}
plot(alldat)
plot(sgdat)
```

So we have lots of stations where fish have been caught and lots of polygons for seagrass.  You'll also notice that the default plotting method for `sf` objects is to create one plot per attribute feature.  This is intended behavior but sometimes is not that useful (it can also break R if you have many attributes).  Maybe we just want to see where the data are located independent of any of the attributes.  We can accomplish this by plotting only the geometry of the `sf` object.

```{r}
plot(alldat$geometry)
plot(sgdat$geometry)
```

To emphasize the point that the `sf` package plays nice with the tidyverse, let's do a simple filter on the fisheries data to look at only the 2016 data. This is the same year as the seagrass data.  

```{r}
filt_dat <- alldat %>% 
  filter(yr == 2016)
plot(filt_dat$geometry)
```

Now let's use the fisheries data and seagrass polygons to do a quick geospatial analysis.  Our simple question is:

__How many fish were caught where seagrass was observed in 2016?_

The first task is to subset the 2016 fisheries data by locations where seagrass was observed.  There are a few ways we can do this.  The first is to make simple subset where we filter the station locations using a spatial overlay with the seagrass polygons  The second and more complete approach is to intersect the two data objects to subset and combine the attributes.

```{r}
fish_crop <- filt_dat[sgdat, ]
plot(fish_crop$geometry)
fish_crop
```

With this first approach we can see which stations were located over seagrass, but we don't know which polygons they're located in for the seagrass data. A better approach is to use `st_intersection` to both overlay and combine the attribute fields from the two data objects.

```{r}
fish_int <- st_intersection(alldat, sgdat)
plot(fish_int$geometry)
fish_int
```

Now we can easily see which stations are in which seagrass polygon.  We can use some familiar tools from dplyr to get the aggregate catch data for the different polygon counts.  Specifically, the `FLUCCS` attribute identifies seagrass polygons as continuous (`9116`) or patchy (`9113`) (metadata [here](https://www.arcgis.com/sharing/rest/content/items/f0ecff0cf0de491685f8fb074adb278b/info/metadata/metadata.xml?format=default&output=html){target="_blank"}.  Maybe we want to summarise species counts by seagrass polygon type.

```{r}
fish_cnt <- fish_int %>% 
  group_by(FLUCCS) %>% 
  summarise(
    cnt = sum(Pinfish)
  ) 
fish_cnt
```

Notice that we've retained the `sf` data structure in the aggregated dataset but the structure is now slightly different, both in the attributes and the geometry column.  We now have only two attributes, `FLUCCS` and `cnt`.  The `geometry` column is retianed but now is aggregated to a multipoint objects where all points in continous or patchy seagrass are grouped by row. This is a really powerful feature of `sf`: spatial attributes are retained during the wrangling process.  

We can also visualize this information with ggplot2. 

```{r, message = F}
ggplot(fish_cnt, aes(x = FLUCCS, y = cnt)) + 
  geom_bar(stat = 'identity')
```

This is not a groundbreaking plot, but we can clearly see that pinfish are more often found in dense seagrass beds (`9116`). 

## Quick mapping 

Cartography or map-making is also very doable in R.  Like most applications, it takes very little time to create something simple, but much more time to create a finished product.  We'll focus on the simple process using `ggplot2` and the `mapview` package just to get you started.  Both packages work "out-of-the-box" with `sf` data objects. 

For `ggplot`, all we need is to use the `geom_sf()` geom.  

```{r}
# use ggplot with sf objects
ggplot() + 
  geom_sf(data = sgdat, fill = 'green') + 
  geom_sf(data = alldat, aes(fill = 'Gear')) 
```

The `mapview` packages lets us create interactive maps to zoom and select data.  Note that we can also combine separate mapview objects with the `+` operator.

```{r}
mapview(sgdat, col.regions = 'green') +
  mapview(alldat, zcol = 'Gear')
```

There’s a lot more we can do with `mapview` but the point is that these maps are incredibly easy to make with `sf` objects and they offer a lot more functionality than static plots.

## Exercise 14

We've certainly covered a lot in this lesson, so we'll spend any remaining time we have on getting more comfortable with basic geospatial analysis and mapping in R.  We'll repeat what we just did above, but selecting different years and only continous seagrass.  

1. Start by filtering the `fish_int` object by the 9116 `FLUCCS` code and only `2016` data.  (hint: use `filter(FLUCSS == 9116)` and then `filter(yr == 2016)`)

1. Create a map of this filtered object using `mapview()`.  Use the `zcol` argument to map the color values to different attributes in your dataset, e.g., `Pinfish` or `Bluefish`.  

1. Try combining the map you made in the last step with one for seagrass (hint: `mapview() + mapview()`).  Did you filter the data correcty, e.g., points are showing only above the continous seagrass polygons? 

<div class="fold s o">
```{r results = 'hide'}
# filter alldat 
tomap <- fish_int %>% 
  filter(FLUCCS == 9116) %>% 
  filter(yr == 2016)

# make map
mapview(tomap, zcol = 'Pinfish')
mapview(tomap, zcol = 'Bluefish')

# join maps
mapview(sgdat) + mapview(tomap, zcol = 'Pinfish')
```
</div>

## Next steps

Now you should be able to: 

1) Understand the vector data structure

1) Understand how to import and structure vector data in R

1) Understand how R stores spatial data using the simple features package

1) Execute basic geospatial functions in R

This concludes our training workshop. I hope you’ve enjoyed the material and found the content useful. Please continue to use this website as a resource for developing your R skills and definitely checkout our [Data and Resources](Data_and_Resources.html) page for additional learning material.  



